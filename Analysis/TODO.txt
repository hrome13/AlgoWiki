(DONE) 1. For ___ Improvements Heatmaps
    - Flip the x-axis (worse on left)
    - Move x-ticks to top

(DONE) 2. Create new heatmap showing only the jumps from first algorithm to best algorithm

3. For the Pareto graphs
    - Draw the frontier with a line ?

(DONE) 4. Create a 100% histogram showing percentage of problems with an optimal algorithm over time
    - See ipad drawing (% with tradeoff, % with optimal)

(DONE) 5. Make the Pareto examples pretty
    - Clean up titles and years
    - denote parallel/quantum algorithms differently

(DONE) 6. Pie charts
    - Recreate the pie charts from Emily's paper but with space (EXCEL)

(DONE) 7. Make the Space Analysis Per Decade chart but with Percentages

(DONE) 8. Histogram of the implied lower bound powers

9. Fix hist_improvements()

10. Fix heat_size_of_improvements()

11. Fix hist_space_and_time_improvements_per_decade()

(DONE) 12. try a version of the pareto examples where you:
    - drop the parallel implementations
    -drop the decade coloring and legend
    -put arrows going from chronologically first to second, second to third, etc.

(DONE) 13. On all versions with a legend, switch 2010’s to 2010s, 2000’s to 2000s, etc.

(DONE?) 14. On versions put “Space Complexity (Augmented)”  @Jayson Lynch: please advise if another label would be better

(DONE) 15. On space analysis per decade:
    -drop decimal places and drop bolding from bar labels
    -use colors in line with charts from time complexity paper
    -cut off y-axis at 100%
    -change title to Percentage of algorithm papers with space complexity analysis (and drop y-axis title)
    -move legend off the bars
    -get the various font sizes more similar

(DONE) 16. For Problems with Space-Time…
    -same changes from other histogram, plus…
    -change title to: Time complexity – space complexity tradeoffs
    -change legend categories to “Single algorithm optimal” and “Time-space tradeoff”
    -is what is shown the flow of new algorithms or the stock of all existing?

(DONE) 17. For Proportion of Algorithms that Improve…
    -Try changing to a pie chart, using same colors as lower bounds paper
    -List as percentages


(DONE) Send Jayson tables for pies

On Domain pages,
    - Change "Related Problems" to "Problems within "Domain"

Add lower bounds
    - Label the lines as lower/upper bounds

Are the bumps from when storage was small and space needed to be optimized?



---------------------------

Determine how to deal with multiple parameters

Calculate input/output sizes

Double check all complexity -> class


Feedback from Neil's presentation
- high level, very interested
- how do we know the questions in the textbook are the right base set?
    - no one has any better alternatives
- each problem is weighted equally.. what about weighting them based on popularity?
    - how often are the wikipedia pages accessed?
    - google trend analysis
    - at the family-level
- maybe also look at the specific algorithm pages, looking at the top 100 algorithms on wikipedia as the base set
- external memory models?
- bioinformatics side project? (might be too deep)


0 - gather the rest

0.001 - categorize into issues
    - missing reference
    - etc.

1 - double check


how many space complexities are non-trivial/interesting?
    - for how many, do we have space = time vs not equal?
    - for how many, space super-linear?
    - trivial vs longer proof? (just looking at the "reference" cell or on overleaf)
        - if proofs are particularly interesting/involved
        - motif search is interesting i think
        - might make an interesting separate paper with a few more involved proofs
    

Paper sections
--------------

Introduction:
    - big picture, why important/care
    - main results

(me / Jayson) Past work:
    - Yash's paper
    - Emily's thesis

(Chirag) Background:
    - what is space complexity?
        - survey in TCS
    - why does it matter?
        - survey in practical computing size

(me) Results:
    - graphs
    - each with a paragraph
        - what it is in detail
        - what analysis went into it
        - sentence or so of discussion/interpretation

Methodology:
    - how these problems were chosen from prior paper  
        - how we chose subsection of problems to consider
    - (Jeffrey) how we gathered space complexity information
    - (me) how we derived our analysis

How to present the overleaf with the derivations or the raw data in some form


Timeline
--------
this week: do all the 0.001 categorizations, send to the group

2-3 weeks


Make a list of assumptions
    - e.g. V = O(E^2)
    - choose the parameter relationship that makes the runtime the "worst", 

Divide matrix multiplication by 2 (in terms of n^2)

Make the "time complexity class" column with multiple parameters like with space

create overleaf for paper



if diff problem is not in sheet1 -> put in new entries

algo not in paper -> double check that algo isn't there (if not, then flag and exclude)

paper for modern computers caring more about space complexity
speed of memory and speed of processing are both increasing exponentially but with different exponentials


asymptotic bucketing
- consistent with first paper
    - round up
- currently implemented
    - use the floats
    - log factors = 0.01
    - loglog factors = 0.001
- what makes most sense under different views
    - polynomials, rounding up makes sense
    - polylog, maybe round down to log?


Start double-checking the '?' ones or the 'looked at? = 1' ones

plans for this week:
finish code
start writing related work part


plans for this week:
Start double-checking the '?' ones or the 'looked at? = 1' ones

plans for next 2 weeks (3/20/23):
Double check the '?' ones or the 'looked at? = 1' ones
Move the ~90 missing problems to the space notes overleaf (also Jeffrey working on this)
Finish writing rough first draft (results, abstract, etc.)





-------------------------------------

April 24th -- full draft done by then.. so Neil and Jayson can read and give edits for a week, and then I do the revisions the week after that

for next week or two, what does the story look like overall



why this is important:
    - increasing number of papers with space complexity analysis
    - memory wall, processors getting faster so memory is becoming the restriction

Big themes:
    1. State of space complexity research
        - what does the landscape look like for space complexity
            - how many have linear, quadratic, etc.?
            - do certain domains have really different distributions than others?
            - overall, do we see a relationship between time and space of algorithms?
            - histogram of improvements per decade
        - Figures:
            - pie chart of constant, linear, superlinear
            - pie chart absolute sizes of auxiliary space
            - pie chart relative size time vs space
            - do these for the domains specifically too^
        - Very rarely see space improvements alone
            - if we have results from the relative pie
            - what percentage of time complexity improvements also showed a space complexity improvement, and vice versa (taking the conditional percentages on the )
                - just a bar chart with two bars
    2. Changes over time
        - space-time tradeoffs
            - does this reflect that in the optimal case, there should be a tradeoff (for some problems, it is known that this tradeoff must exist)
        - add discussion about why this is true (why do we see these tradeoffs?)
    3. Rates of improvements
        - rate of space improvement compared to the rate of improvement in time
            - do this with multiple different n (1000, 10^6, 10^9)
            - scatter plot, one dimension is time complexity improvement, other is space complexity improvement
        - in hardware, memory is increasing exponentially
            - space is also increasing exponentially but slower
            - there is now a big gap b/w main memory and processor
            - not only is hardware gap increasing, but also in theory we aren't getting as much gains in space
            - for both time and space, we have hardware improvements (processor better(moore's law) or memory better), and efficiency improvements (algorithms)
                - processors improving faster than memory, implies bottleneck in memory (memory wall)
                - if space also isn't improving as fast as time, then this gap in improvements is similar to the gap in hardware
                - on the other hand, if space is improving faster than time, then the gap in hardware is being compensated by the gap in theory


for next week, do some of these analyses and sketch up some writing for them